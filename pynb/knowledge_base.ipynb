{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d4e00929-85cf-49d3-8132-cc20968b3c67",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Explicitly passing a `revision` is encouraged when loading a configuration with custom code to ensure no malicious code has been contributed in a newer revision.\n",
      "Explicitly passing a `revision` is encouraged when loading a model with custom code to ensure no malicious code has been contributed in a newer revision.\n",
      "Explicitly passing a `revision` is encouraged when loading a model with custom code to ensure no malicious code has been contributed in a newer revision.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c06ae65b7cf481e9b3b11c96c45ea11",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.docstore.document import Document\n",
    "from langchain.chains.summarize import load_summarize_chain\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.chains import LLMRequestsChain, LLMChain\n",
    "from langchain import LLMChain\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.chains.mapreduce import MapReduceChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.llms.base import LLM\n",
    "from transformers import AutoTokenizer, AutoModel, AutoConfig\n",
    "from typing import Any, Dict, List, Mapping, Optional, Tuple, Union\n",
    "from torch.mps import empty_cache\n",
    "import torch\n",
    "import sys\n",
    "\n",
    "class GLM(LLM):\n",
    "    max_token: int = 2048\n",
    "    temperature: float = 0.8\n",
    "    top_p = 0.9\n",
    "    tokenizer: object = None\n",
    "    model: object = None\n",
    "    history_len: int = 1024\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    @property\n",
    "    def _llm_type(self) -> str:\n",
    "        return \"GLM\"\n",
    "\n",
    "    def load_model(self, llm_device=\"gpu\",model_name_or_path=None):\n",
    "        model_config = AutoConfig.from_pretrained(model_name_or_path, trust_remote_code=True)\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name_or_path,trust_remote_code=True)\n",
    "        self.model = AutoModel.from_pretrained(model_name_or_path, config=model_config, trust_remote_code=True).half().cuda()\n",
    "\n",
    "    def _call(self,prompt:str,history:List[str] = [],stop: Optional[List[str]] = None):\n",
    "        response, _ = self.model.chat(\n",
    "                    self.tokenizer,prompt,\n",
    "                    history=history[-self.history_len:] if self.history_len > 0 else [],\n",
    "                    max_length=self.max_token,temperature=self.temperature,\n",
    "                    top_p=self.top_p)\n",
    "        return response\n",
    "\n",
    "    \n",
    "modelpath = \"/root/model/chatglm-6b\"\n",
    "sys.path.append(modelpath)\n",
    "llm = GLM()\n",
    "llm.load_model(model_name_or_path = modelpath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "50fa2b51-a39c-41d0-bf75-b10a1f340235",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQA #检索QA链，在文档上进行检索\n",
    "from langchain.document_loaders import CSVLoader #文档加载器，采用csv格式存储\n",
    "from langchain.vectorstores import DocArrayInMemorySearch #向量存储\n",
    "from IPython.display import display, Markdown #在jupyter显示信息的工具\n",
    "\n",
    "file = 'clothing-short.csv'\n",
    "loader = CSVLoader(file_path=file)\n",
    "\n",
    "import pandas as pd\n",
    "data = pd.read_csv(file,header=None)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6b814704-dd04-4452-9d8d-0ad7f3e953fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>name</td>\n",
       "      <td>description</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>Women's Campside Oxfords</td>\n",
       "      <td>This ultracomfortable lace-to-toe Oxford boast...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>Recycled Waterhog Dog Mat, Chevron Weave</td>\n",
       "      <td>Protect your floors from spills and splashing ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2.0</td>\n",
       "      <td>Infant and Toddler Girls' Coastal Chill Swimsu...</td>\n",
       "      <td>She'll love the bright colors, ruffles and exc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3.0</td>\n",
       "      <td>Refresh Swimwear, V-Neck Tankini Contrasts</td>\n",
       "      <td>Whether you're going for a swim or heading out...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>995.0</td>\n",
       "      <td>Men's  Classic Denim, Standard Fit</td>\n",
       "      <td>Crafted from premium denim that will last wash...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>996.0</td>\n",
       "      <td>CozyPrint Sweater Fleece Pullover</td>\n",
       "      <td>The ultimate sweater fleece - made from superi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>997.0</td>\n",
       "      <td>Women's NRS Endurance Spray Paddling Pants</td>\n",
       "      <td>These comfortable and affordable splash paddli...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>998.0</td>\n",
       "      <td>Women's Stop Flies Hoodie</td>\n",
       "      <td>This great-looking hoodie uses No Fly Zone Tec...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1000</th>\n",
       "      <td>999.0</td>\n",
       "      <td>Modern Utility Bag</td>\n",
       "      <td>This US-made crossbody bag is built with the s...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1001 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          0                                                  1  \\\n",
       "0       NaN                                               name   \n",
       "1       0.0                           Women's Campside Oxfords   \n",
       "2       1.0           Recycled Waterhog Dog Mat, Chevron Weave   \n",
       "3       2.0  Infant and Toddler Girls' Coastal Chill Swimsu...   \n",
       "4       3.0         Refresh Swimwear, V-Neck Tankini Contrasts   \n",
       "...     ...                                                ...   \n",
       "996   995.0                 Men's  Classic Denim, Standard Fit   \n",
       "997   996.0                  CozyPrint Sweater Fleece Pullover   \n",
       "998   997.0         Women's NRS Endurance Spray Paddling Pants   \n",
       "999   998.0                          Women's Stop Flies Hoodie   \n",
       "1000  999.0                                 Modern Utility Bag   \n",
       "\n",
       "                                                      2  \n",
       "0                                           description  \n",
       "1     This ultracomfortable lace-to-toe Oxford boast...  \n",
       "2     Protect your floors from spills and splashing ...  \n",
       "3     She'll love the bright colors, ruffles and exc...  \n",
       "4     Whether you're going for a swim or heading out...  \n",
       "...                                                 ...  \n",
       "996   Crafted from premium denim that will last wash...  \n",
       "997   The ultimate sweater fleece - made from superi...  \n",
       "998   These comfortable and affordable splash paddli...  \n",
       "999   This great-looking hoodie uses No Fly Zone Tec...  \n",
       "1000  This US-made crossbody bag is built with the s...  \n",
       "\n",
       "[1001 rows x 3 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.indexes import VectorstoreIndexCreator\n",
    "from langchain.embeddings import SentenceTransformerEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "\n",
    "from IPython.display import display, Markdown \n",
    "embeddings = SentenceTransformerEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "data = loader.load()\n",
    "query =\"Please list all your shirts with sun protection \\\n",
    "in a table in markdown and summarize each one.\"\n",
    "\n",
    "db = FAISS.from_documents(data, embeddings)\n",
    "docs = db.similarity_search(query)\n",
    "\n",
    "\n",
    "db = DocArrayInMemorySearch.from_documents(docs, embeddings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6ec4328e-20f6-4c50-a277-ef5d1220535e",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValidationError",
     "evalue": "1 validation error for OpenAIEmbeddings\n__root__\n  Did not find openai_api_key, please add an environment variable `OPENAI_API_KEY` which contains it, or pass  `openai_api_key` as a named parameter. (type=value_error)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValidationError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mindexes\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m VectorstoreIndexCreator\n\u001b[0;32m----> 2\u001b[0m index \u001b[38;5;241m=\u001b[39m \u001b[43mVectorstoreIndexCreator\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvectorstore_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mDocArrayInMemorySearch\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mfrom_loaders([loader])\n\u001b[1;32m      6\u001b[0m query \u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease list all your shirts with sun protection \u001b[39m\u001b[38;5;130;01m\\\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;124min a table in markdown and summarize each one.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      9\u001b[0m response \u001b[38;5;241m=\u001b[39m index\u001b[38;5;241m.\u001b[39mquery(query)\n",
      "File \u001b[0;32m~/autodl-tmp/py310/lib/python3.10/site-packages/pydantic/main.py:339\u001b[0m, in \u001b[0;36mpydantic.main.BaseModel.__init__\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/autodl-tmp/py310/lib/python3.10/site-packages/pydantic/main.py:1066\u001b[0m, in \u001b[0;36mpydantic.main.validate_model\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/autodl-tmp/py310/lib/python3.10/site-packages/pydantic/fields.py:439\u001b[0m, in \u001b[0;36mpydantic.fields.ModelField.get_default\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/autodl-tmp/py310/lib/python3.10/site-packages/pydantic/main.py:341\u001b[0m, in \u001b[0;36mpydantic.main.BaseModel.__init__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mValidationError\u001b[0m: 1 validation error for OpenAIEmbeddings\n__root__\n  Did not find openai_api_key, please add an environment variable `OPENAI_API_KEY` which contains it, or pass  `openai_api_key` as a named parameter. (type=value_error)"
     ]
    }
   ],
   "source": [
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.vectorstores import Chroma\n",
    "\n",
    "\n",
    "with open(\"pirlo.txt\") as f:\n",
    "    book = f.read()\n",
    "\n",
    "text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
    "texts = text_splitter.split_text(book)\n",
    "docsearch = Chroma.from_texts(\n",
    "    texts, embeddings, metadatas=[{\"source\": str(i)} for i in range(len(texts))]\n",
    ")\n",
    "\n",
    "\n",
    "query = \"皮尔洛是谁?\"\n",
    "docs = docsearch.similarity_search_with_score(query, k=1)\n",
    "for doc in docs:\n",
    "    print(doc)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9912d2f2-c96b-4905-990e-66ba044f326a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py310",
   "language": "python",
   "name": "py310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
